## Importation librairies

library("lda")
library('mixer')
require(gtools)
require(plotrix)

library("ergm")
library("network")
library("igraph")
library("RColorBrewer") ### pour colorer nos graphes
library("flexclust")
library("sand")


## on importe les données
setwd("/Users/arnaudb/Desktop/sémi stat")
graph_twitter = read.csv ("adja_1000.csv", sep=";" , header = TRUE)
rownames(graph_twitter)=graph_twitter[,1]
graph_twitter=as.matrix(graph_twitter[,-1])
colnames(graph_twitter) <- rownames(graph_twitter)

infos_twitter = read.csv("donnees_twitter_1000.csv", sep = ";", header = TRUE)

party_twitter=infos_twitter[,3]



### à executer si l'on veut rééquilibrer le graphe
indices_mel=which(party_twitter %in% "#JLM2017")
extrait_mel=indices_mel[sample(362,250)]
indices_notpar=which(party_twitter %in% "Not partisan")
extrait_notpar=indices_notpar[sample(313,190)]

extrait=c(extrait_mel, extrait_notpar)

graph_twitter_reduit=graph_twitter[-extrait,-extrait]
party_twitter_reduit=party_twitter[-extrait]


## À executer si l'on veut augmenter la densité du graphe

indices_mel=which(party_twitter_reduit %in% "#JLM2017")
indices_fil=which(party_twitter_reduit %in% "#Fillon2017")
indices_mac=which(party_twitter_reduit %in% "#Macron2017")
indices_ham=which(party_twitter_reduit %in% "#Hamon2017")
indices_mlp=which(party_twitter_reduit %in% "#Marine2017")

graphe_cheat=graph_twitter_reduit

cheat_code <- function(l,p, graphe){
  modif=graphe
  for (i in 1:length(l)){
    for (j in 1:length(l)){
      if (runif(1) < p){
        modif[l[i],l[j]]=1
      }
    }
  }
  return(modif)
}

graphe_cheat=cheat_code(indices_mel, 0.7, graphe_cheat)
sum(graphe_cheat)
graphe_cheat=cheat_code(indices_fil, 0.6, graphe_cheat)
sum(graphe_cheat)
graphe_cheat=cheat_code(indices_ham, 0.6, graphe_cheat)
sum(graphe_cheat)
graphe_cheat=cheat_code(indices_mlp, 0.5, graphe_cheat)
sum(graphe_cheat)
graphe_cheat=cheat_code(indices_mac, 0.4, graphe_cheat)
sum(graphe_cheat)



## Quelle densitée de la matrice?
print(paste("Densité de la matrice : ", sum(graph_twitter)/(1000*1000)))


## on cherche a retrouver les partis politiques
K=10

model_twitter_25 <- mmsb.collapsed.gibbs.sampler(graph_twitter,
                                      K = K, num.iterations=500,
                                      alpha = 0.25,
                                      burnin = 20L,
                                      beta.prior = list(diag(8,K)+1, diag(5, K) + 1))

model_twitter_01 <- mmsb.collapsed.gibbs.sampler(graph_twitter,
                                                 K = K, num.iterations=500,
                                                 alpha = 0.1,
                                                 burnin = 20L,
                                                 beta.prior = list(diag(8,K)+1, diag(5, K) + 1))


#on retrouve les estimations de B et de pi
memhat_25 <- with(model_twitter_25, t(document_sums) / colSums(document_sums))
Bhat_25 <-  with(model_twitter_25, blocks.pos / (blocks.pos + blocks.neg))

esp_25 = memhat_25 %*% Bhat_25 %*% t(memhat_25)

memhat_01 <- with(model_twitter_01, t(model_twitter_01$document_sums) / colSums(model_twitter_01$document_sums))
Bhat_01 <-  with(model_twitter_01, model_twitter_01$blocks.pos / (model_twitter_01$blocks.pos + model_twitter_01$blocks.neg))

esp_01 = memhat_01 %*% Bhat_01 %*% t(memhat_01)

#on affiche le graphe des blocks
reordo_twitter <- matrix(0,nrow(graph_twitter),1)
for (k in 1:nrow(graph_twitter_reduit)){
  reordo_twitter[k,1]=which.max(memhat_01[k,])
}

recon=esp_01[order(reordo_twitter), order(reordo_twitter)]
par(mfrow=c(1,1))
image(1-recon, col= grey.colors(30, start = 0.1, end = 1),  xlab = "noeuds", ylab="noeuds" )


#on affiche la relation entre les blocks

color2D.matplot(Bhat_25, show.values=3)
color2D.matplot(Bhat_01, show.values=3)

#On regarde la performance à retrouver les partis

list_partis_twitter=list()
for (i in 1:K){
  list_partis_twitter[[i]]=party_twitter[reordo_twitter==i]
}

par(mfrow=c(3,3))
for (k in 1:9){
  plot(table(list_partis_twitter[k]), main=paste("Classe numéro ",k), xlab="partis", ylab="nombre d'utilisateurs", lwd=6 )
}
par(mfrow=c(1,1))
plot(table(list_partis_twitter[10]), main=paste("Classe numéro ",k), xlab="partis", ylab="nombre d'utilisateurs", lwd=6 )


### projection sur le polygone

color_list_twitter=c("#984EA3","#FFFF33","#377EB8","#E41A1C","#A65628","#FF7F00","#4DAF4A","#7FFF00","#F781BF","#999999")


coord_polygone_regulier=function(n){
  list_x=c()
  list_y=c()
  res=list()
  for (k in (1:n)){
    list_x=c(list_x,cos(2*k*pi/n))
    list_y=c(list_y,sin(2*k*pi/n))
  }
  res[[1]]=list_x
  res[[2]]=list_y
  return(res)
}

vecteur_coordonne=function(n){
  res=coord_polygone_regulier(n)
  matrix_res=matrix(0,n,2)
  for (i in 1:n){
    
    matrix_res[i,]=as.matrix(c(res[[1]][i],res[[2]][i]))
  } 
  return(matrix_res)
}

plot_sur_polygone_twitter=function(n, matrice_proba){
  coord=vecteur_coordonne(n)
  res_plot=coord_polygone_regulier(n)
  res_plot[[1]]=c(res_plot[[1]],res_plot[[1]][1])
  res_plot[[2]]=c(res_plot[[2]],res_plot[[2]][1])
  vecteur_color=color_list[party.nums]
  coord=vecteur_coordonne(n)
  atracer=matrice_proba%*%coord
  
  #cl1 = kcca(atracer, k=n, kccaFamily("kmeans"))
  #image(cl1)
  #par(new=TRUE)
  plot(res_plot[[1]],res_plot[[2]],"l",xlab="",ylab="", lwd=3, xlim=c(-1.3, 1.1), ylim=c(-1.1, 1.3) )
  for (m in 1:n){
    lines(c(0,res_plot[[1]][m]), c(0,res_plot[[2]][m]), "l", lty=2, lwd=0.9, col="gray90")
  }
  points(atracer,col=color_list_twitter[infos_twitter[,3]], pch=20, cex=0.8)
  
  centroids=matrix(0,length(list_partis),K)
  for (k in 1:length(party.names)){
    centroids[k,]=colMeans(matrice_proba[ party.nums == k , ])
  }
  coord_centro= centroids %*% coord
  points(coord_centro, col=color_list_twitter[], pch=15, cex=2)
  
}


par(mfrow=c(1,1))
plot_sur_polygone_twitter(10, memhat_01)
legend(x=-1.35, y=1.25, legend = levels(infos_twitter[,3]), col=color_list_twitter, pch=20, text.col = color_list_twitter, text.width = 0.4, x.intersp	
       =0.5, cex=0.7)

